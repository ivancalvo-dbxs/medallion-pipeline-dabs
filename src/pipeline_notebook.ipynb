{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6d9d51",
   "metadata": {},
   "source": [
    "# Pipeline and Medallion Architecture\n",
    "\n",
    "In this notebook we are going to combine the ideas from:\n",
    "- [Example medallion architecture](https://docs.databricks.com/aws/en/lakehouse/medallion#example-medallion-architecture)\n",
    "- [Load data with Lakeflow Declarative Pipelines\n",
    "](https://docs.databricks.com/aws/en/ldp/load)\n",
    "- [Manage data quality with pipeline expectations\n",
    "](https://docs.databricks.com/aws/en/ldp/expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark import pipelines as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c83fec",
   "metadata": {},
   "source": [
    "# Landing zone directories to read the raw data\n",
    "\n",
    "- In this example we are using the fake generated data stored in the Managed Volume.\n",
    "- In a real scenario, could be:\n",
    "    - Cloud-object storage path: S3, ADLS, GCS\n",
    "    - External Volume path: Access existing Clod-object storage using volume-like paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f65586",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = spark.conf.get(\"landing_zone_volume\", \"\")\n",
    "bronze_schema = spark.conf.get(\"bronze_schema\", \"\")\n",
    "silver_schema = spark.conf.get(\"silver_schema\", \"\")\n",
    "gold_schema = spark.conf.get(\"gold_schema\", \"\")\n",
    "\n",
    "customers_directory = f\"{volume}/customers\"\n",
    "products_directory = f\"{volume}/products\"\n",
    "transactions_directory = f\"{volume}/transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138135a0",
   "metadata": {},
   "source": [
    "# Bronze Tables\n",
    "\n",
    "In the bronze layer we are supposed to only:\n",
    "\n",
    "- Load the raw data into tables\n",
    "- Avoid transformations, changes and filters in the data.\n",
    "- Keep the data in the original format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5b080",
   "metadata": {},
   "source": [
    "### Customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d16a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@dp\u001b[39m\u001b[38;5;241m.\u001b[39mtable(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbronze_schema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.customers_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbronze_customers\u001b[39m():\n\u001b[1;32m      3\u001b[0m   df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m       spark\u001b[38;5;241m.\u001b[39mreadStream\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloudFiles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;241m.\u001b[39mload(customers_directory)\n\u001b[1;32m      9\u001b[0m   )\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dp' is not defined"
     ]
    }
   ],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.customers_raw\")\n",
    "def bronze_customers():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the customer data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"customer_id STRING, name STRING, country STRING, registration_date DATE, customer_segment STRING\"\n",
    "  \n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"csv\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(customers_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf3267",
   "metadata": {},
   "source": [
    "### Products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.products_raw\")\n",
    "def bronze_products():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"product_id STRING, product_name STRING, category STRING, price DOUBLE, cost DOUBLE\"\n",
    "\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"json\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(products_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30428d52",
   "metadata": {},
   "source": [
    "### Transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf87dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.transactions_raw\")\n",
    "def bronze_products():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"transaction_id STRING, customer_id STRING, product_id STRING, quantity_id STRING, category STRING, price DOUBLE, cost DOUBLE\"\n",
    "\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(transactions_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf1c81",
   "metadata": {},
   "source": [
    "## Silver Tables\n",
    "\n",
    "In this stage yo are supposed to:\n",
    "    - Clean the data.\n",
    "    - Transform the data.\n",
    "    - Apply business and data quality rules to the data.\n",
    "\n",
    "The only rule in this example is that there cannot be nulls on the downstream, therefore, we'll exclude the records with this criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca5c8b",
   "metadata": {},
   "source": [
    "### Create silver customers table with an expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a286cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "@dp.table(name=f\"{silver_schema}.customers_cleaned\")\n",
    "@dp.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "def silver_customers():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.customers_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef64991",
   "metadata": {},
   "source": [
    "### Create silver orders table with an expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ed0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@dp.table(name=f\"{silver_schema}.orders_cleaned\")\n",
    "@dp.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "def silver_orders():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.orders_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.\n",
    "  (\"_rescued_data\")\n",
    "  return df\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a28c6",
   "metadata": {},
   "source": [
    "### Join the silver tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@dp.table(name=f\"{silver_schema}.customers_orders\")\n",
    "def customer_orders():\n",
    "    # Silver tables\n",
    "    customers_cleaned_df = spark.readStream.table(f\"{silver_schema}.customers_cleaned\")\n",
    "    orders_cleaned_df = spark.readStream.table(f\"{silver_schema}.orders_cleaned\")\n",
    "\n",
    "    # Join\n",
    "    df = customers_cleaned_df.join(orders_cleaned_df, on=\"customer_id\", how=\"inner\")\n",
    "    return df\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
