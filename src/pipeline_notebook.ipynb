{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6d9d51",
   "metadata": {},
   "source": [
    "# Pipeline and Medallion Architecture\n",
    "\n",
    "In this notebook we are going to combine the ideas from:\n",
    "- [Example medallion architecture](https://docs.databricks.com/aws/en/lakehouse/medallion#example-medallion-architecture)\n",
    "- [Load data with Lakeflow Declarative Pipelines\n",
    "](https://docs.databricks.com/aws/en/ldp/load)\n",
    "- [Manage data quality with pipeline expectations\n",
    "](https://docs.databricks.com/aws/en/ldp/expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark import pipelines as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c83fec",
   "metadata": {},
   "source": [
    "# Landing zone directories to read the raw data\n",
    "\n",
    "- In this example we are using the fake generated data stored in the Managed Volume.\n",
    "- In a real scenario, could be:\n",
    "    - Cloud-object storage path: S3, ADLS, GCS\n",
    "    - External Volume path: Access existing Clod-object storage using volume-like paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f65586",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = spark.conf.get(\"landing_zone_volume\", \"\")\n",
    "bronze_schema = spark.conf.get(\"bronze_schema\", \"\")\n",
    "silver_schema = spark.conf.get(\"silver_schema\", \"\")\n",
    "gold_schema = spark.conf.get(\"gold_schema\", \"\")\n",
    "\n",
    "customers_directory = f\"{volume}/customers\"\n",
    "products_directory = f\"{volume}/products\"\n",
    "transactions_directory = f\"{volume}/transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138135a0",
   "metadata": {},
   "source": [
    "# Bronze Tables\n",
    "\n",
    "In the bronze layer we are supposed to only:\n",
    "\n",
    "- Load the raw data into tables\n",
    "- Avoid transformations, changes and filters in the data.\n",
    "- Keep the data in the original format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5b080",
   "metadata": {},
   "source": [
    "### Customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d16a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@dp\u001b[39m\u001b[38;5;241m.\u001b[39mtable(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbronze_schema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.customers_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbronze_customers\u001b[39m():\n\u001b[1;32m      3\u001b[0m   df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m       spark\u001b[38;5;241m.\u001b[39mreadStream\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloudFiles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[38;5;241m.\u001b[39mload(customers_directory)\n\u001b[1;32m      9\u001b[0m   )\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dp' is not defined"
     ]
    }
   ],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.customers_raw\")\n",
    "def bronze_customers():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the customer data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"customer_id STRING, name STRING, country STRING, registration_date DATE, customer_segment STRING\"\n",
    "  \n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(customers_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf3267",
   "metadata": {},
   "source": [
    "### Products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.products_raw\")\n",
    "def bronze_products():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"product_id STRING, product_name STRING, category STRING, price DOUBLE, cost DOUBLE\"\n",
    "\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(products_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30428d52",
   "metadata": {},
   "source": [
    "### Transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf87dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.transactions_raw\")\n",
    "def bronze_products():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "\n",
    "  SCHEMA_HINTS = \"transaction_id STRING, customer_id STRING, product_id STRING, quantity_id STRING, category STRING, price DOUBLE, cost DOUBLE\"\n",
    "\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .option(\"cloudFiles.schemaHints\", SCHEMA_HINTS)\n",
    "          .load(transactions_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf1c81",
   "metadata": {},
   "source": [
    "## Silver Tables\n",
    "\n",
    "In this stage yo are supposed to:\n",
    "    - Clean the data.\n",
    "    - Transform the data.\n",
    "    - Apply business and data quality rules to the data.\n",
    "\n",
    "The only rule in this example is that there cannot be nulls on the downstream, therefore, we'll exclude the records with this criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca5c8b",
   "metadata": {},
   "source": [
    "### Customers table with a data quality expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a286cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.customers\")\n",
    "@dp.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "def silver_customers():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.customers_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef64991",
   "metadata": {},
   "source": [
    "### Products table with a data quality expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ed0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.products\")\n",
    "@dp.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "def silver_products():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.products_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fe78e",
   "metadata": {},
   "source": [
    "### Transactions table with a data quality expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.transactions\")\n",
    "@dp.expect_or_drop(\"valid_transaction_id\", \"transaction_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "def silver_transactions():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.transactions_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5a3d9",
   "metadata": {},
   "source": [
    "## Sales detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.sales_detail\")\n",
    "def sales_detail():\n",
    "    # Silver tables\n",
    "    customers_df = spark.readStream.table(f\"{silver_schema}.customers\")\n",
    "    products_df = spark.readStream.table(f\"{silver_schema}.products\")\n",
    "    transactions_df = spark.readStream.table(f\"{silver_schema}.transactions\")\n",
    "\n",
    "    sales_detail_df = transactions_df \\\n",
    "    .join(customers_df, on='customer_id') \\\n",
    "    .join(products_df, on='product_id')\n",
    "\n",
    "    sales_detail_df = sales_detail_df.withColumn('revenue', F.col('price') * F.col('quantity') * (1 - col('discount_applied')))\n",
    "\n",
    "    return sales_detail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a28c6",
   "metadata": {},
   "source": [
    "## Sales per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.revenue_transactions\")\n",
    "def revenue_transactions():\n",
    "    # Silver tables\n",
    "    sales_detail_df = spark.readStream.table(f\"{silver_schema}.sales_detail\")\n",
    "    \n",
    "    revenue_transactions_df = sales_detail_df.groupBy('country').agg(\n",
    "        F.round(F.sum('revenue'), 2).alias('total_revenue'),\n",
    "        F.count('transaction_id').alias('total_transactions'),\n",
    "        F.countDistinct('customer_id').alias('unique_customers')\n",
    "    )\n",
    "\n",
    "    revenue_transactions_df = revenue_transactions_df.withColumn(\n",
    "    'avg_order_value', F.round(F.col('total_revenue') / F.col('total_transactions'), 2))\n",
    "\n",
    "    return revenue_transactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c99e76",
   "metadata": {},
   "source": [
    "## Segments per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.segments_per_country\")\n",
    "def segments_per_country():\n",
    "\n",
    "    sales_detail_df = spark.readStream.table(f\"{silver_schema}.sales_detail\")\n",
    "\n",
    "    segment_per_country = sales_detail_df.groupBy('country', 'customer_segment') \\\n",
    "    .agg(F.count('*').alias('segment_count')) \\\n",
    "    .withColumn('rank', F.row_number().over(F.Window.partitionBy('country').orderBy(F.desc('segment_count')))) \\\n",
    "    .filter(F.col('rank') == 1) \\\n",
    "    .select('country', F.col('customer_segment').alias('top_customer_segment'))\n",
    "\n",
    "    return segment_per_country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457dc12f",
   "metadata": {},
   "source": [
    "# Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{gold_schema}.sales_per_country\")\n",
    "def sales_per_country():\n",
    "\n",
    "    revenue_transactions_df = spark.readStream.table(f\"{silver_schema}.revenue_transactions\")\n",
    "    segments_per_country_df = spark.readStream.table(f\"{silver_schema}.segments_per_country\")\n",
    "\n",
    "    sales_per_country_df = revenue_transactions_df.join(segments_per_country_df, on='country').orderBy(F.desc('total_revenue'))\n",
    "\n",
    "    return sales_per_country_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
