{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6d9d51",
   "metadata": {},
   "source": [
    "# Pipeline and Medallion Architecture\n",
    "\n",
    "In this notebook we are going to combine the ideas from:\n",
    "- [Example medallion architecture](https://docs.databricks.com/aws/en/lakehouse/medallion#example-medallion-architecture)\n",
    "- [Load data with Lakeflow Declarative Pipelines\n",
    "](https://docs.databricks.com/aws/en/ldp/load)\n",
    "- [Manage data quality with pipeline expectations\n",
    "](https://docs.databricks.com/aws/en/ldp/expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import pipelines as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c83fec",
   "metadata": {},
   "source": [
    "# Landing zone (directories where the raw data is)\n",
    "\n",
    "- In this example we are using the fake generated data stored in the Managed Volume.\n",
    "- In a real scenario, could be:\n",
    "    - Cloud-object storage path: S3, ADLS, GCS\n",
    "    - External Volume path: Access existing Clod-object storage using volume-like paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f65586",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = spark.conf.get(\"landing_zone_volume\", \"\")\n",
    "bronze_schema = spark.conf.get(\"bronze_schema\", \"\")\n",
    "silver_schema = spark.conf.get(\"silver_schema\", \"\")\n",
    "gold_schema = spark.conf.get(\"gold_schema\", \"\")\n",
    "\n",
    "customers_directory = f\"{volume}/customers\"\n",
    "products_directory = f\"{volume}/products\"\n",
    "transactions_directory = f\"{volume}/transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138135a0",
   "metadata": {},
   "source": [
    "# Bronze layer\n",
    "\n",
    "In the bronze layer we are supposed to only:\n",
    "\n",
    "- Load the parquet raw data into tables\n",
    "- Avoid transformations, changes and filters in the data.\n",
    "- Keep the data in the original format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5b080",
   "metadata": {},
   "source": [
    "### Customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d16a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.customers_raw\")\n",
    "def bronze_customers():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the customer data\n",
    "  \"\"\"\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .load(customers_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf3267",
   "metadata": {},
   "source": [
    "### Products table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.products_raw\")\n",
    "def bronze_products():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .load(products_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30428d52",
   "metadata": {},
   "source": [
    "### Transactions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf87dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{bronze_schema}.transactions_raw\")\n",
    "def bronze_transactions():\n",
    "  \"\"\"\n",
    "    Returns a Spark Streaming Dataframe that uses AutoLoader to incrementally read the orders data\n",
    "  \"\"\"\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"parquet\")\n",
    "          .load(transactions_directory)\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf1c81",
   "metadata": {},
   "source": [
    "## Silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca5c8b",
   "metadata": {},
   "source": [
    "### Customers table with a data quality expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a286cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.customers\")\n",
    "@dp.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "def silver_customers():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.customers_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef64991",
   "metadata": {},
   "source": [
    "### Products table with a data quality expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ed0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.products\")\n",
    "@dp.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "def silver_products():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.products_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fe78e",
   "metadata": {},
   "source": [
    "### Transactions table with a data quality expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dec02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.transactions\")\n",
    "@dp.expect_or_drop(\"valid_transaction_id\", \"transaction_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_product_id\", \"product_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dp.expect_or_drop(\"valid_quantity\", \"quantity IS NOT NULL\")\n",
    "def silver_transactions():\n",
    "  # Read Bronze table\n",
    "  df = spark.readStream.table(f\"{bronze_schema}.transactions_raw\")\n",
    "\n",
    "  # Drop the Auto Loader generated column, no longer needed on silver.\n",
    "  df = df.drop(\"_rescued_data\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5a3d9",
   "metadata": {},
   "source": [
    "### Sales detail (join of 3 previous tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(name=f\"{silver_schema}.sales_detail\")\n",
    "@dp.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "@dp.expect_or_drop(\"valid_unit_price\", \"unit_price > 0\")\n",
    "@dp.expect_or_drop(\"valid_country\", \"country IS NOT NULL\")\n",
    "def silver_sales_detail():\n",
    "    # Silver tables\n",
    "    customers_df = spark.readStream.table(f\"{silver_schema}.customers\")\n",
    "    products_df = spark.readStream.table(f\"{silver_schema}.products\")\n",
    "    transactions_df = spark.readStream.table(f\"{silver_schema}.transactions\")\n",
    "\n",
    "    silver_sales_detail_df = transactions_df \\\n",
    "    .join(customers_df, \"customer_id\", \"inner\") \\\n",
    "    .join(products_df, \"product_id\", \"inner\") \\\n",
    "    .select(\n",
    "        F.col(\"transaction_id\"),\n",
    "        F.col(\"transaction_date\"),\n",
    "        F.year(F.col(\"transaction_date\")).alias(\"transaction_year\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"name\").alias(\"customer_name\"),\n",
    "        F.col(\"email\").alias(\"customer_email\"),\n",
    "        F.col(\"country\"),\n",
    "        F.col(\"customer_segment\"),\n",
    "        F.col(\"product_id\"),\n",
    "        F.col(\"product_name\"),\n",
    "        F.col(\"category\").alias(\"product_category\"),\n",
    "        F.col(\"price\").alias(\"unit_price\"),\n",
    "        F.col(\"cost\").alias(\"unit_cost\"),\n",
    "        F.col(\"quantity\"),\n",
    "        # Calculated columns\n",
    "        F.round(F.col(\"price\") * F.col(\"quantity\"), 2).alias(\"total_sales\"),\n",
    "        F.round(F.col(\"cost\") * F.col(\"quantity\"), 2).alias(\"total_cost\"),\n",
    "        F.round((F.col(\"price\") - F.col(\"cost\")) * F.col(\"quantity\"), 2).alias(\"total_profit\"),\n",
    "        F.round(((F.col(\"price\") - F.col(\"cost\")) / F.col(\"price\")) * 100, 2).alias(\"profit_margin_pct\")\n",
    "    )\n",
    "    return silver_sales_detail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457dc12f",
   "metadata": {},
   "source": [
    "# Gold layer (business aggregates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.materialized_view(name=f\"{gold_schema}.sales_by_country\")\n",
    "def gold_sales_by_country():\n",
    "\n",
    "    sales_detail_df = spark.read.table(f\"{silver_schema}.sales_detail\")\n",
    "    sales_by_country_df = sales_detail_df \\\n",
    "        .groupBy(\"country\") \\\n",
    "        .agg(\n",
    "            F.sum(\"total_sales\").alias(\"total_sales\"),\n",
    "            F.sum(\"total_cost\").alias(\"total_cost\"),\n",
    "            F.sum(\"total_profit\").alias(\"total_revenue\"),\n",
    "            F.count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "            F.round(F.avg(\"total_sales\"), 2).alias(\"avg_transaction_value\"),\n",
    "            F.count(\"customer_id\").alias(\"customer_count\")\n",
    "        ) \\\n",
    "        .withColumn(\"total_sales\", F.round(F.col(\"total_sales\"), 2)) \\\n",
    "        .withColumn(\"total_cost\", F.round(F.col(\"total_cost\"), 2)) \\\n",
    "        .withColumn(\"total_revenue\", F.round(F.col(\"total_revenue\"), 2)) \\\n",
    "        .orderBy(F.col(\"total_revenue\").desc())\n",
    "    \n",
    "    return sales_by_country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189753f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.materialized_view(name=f\"{gold_schema}.revenue_per_year\")\n",
    "def gold_revenue_per_year():\n",
    "\n",
    "    sales_detail_df = spark.read.table(f\"{silver_schema}.sales_detail\")\n",
    "\n",
    "    revenue_by_year_df = sales_detail_df \\\n",
    "    .groupBy(\"transaction_year\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_sales\").alias(\"total_sales\"),\n",
    "        F.sum(\"total_cost\").alias(\"total_cost\"),\n",
    "        F.sum(\"total_profit\").alias(\"total_revenue\"),\n",
    "        F.count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        F.count(\"customer_id\").alias(\"unique_customers\"),\n",
    "        F.round(F.avg(\"total_sales\"), 2).alias(\"avg_transaction_value\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_sales\", F.round(F.col(\"total_sales\"), 2)) \\\n",
    "    .withColumn(\"total_cost\", F.round(F.col(\"total_cost\"), 2)) \\\n",
    "    .withColumn(\"total_revenue\", F.round(F.col(\"total_revenue\"), 2)) \\\n",
    "    .orderBy(\"transaction_year\")\n",
    "\n",
    "    return revenue_by_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.materialized_view(name=f\"{gold_schema}.customer_segment_performance\")\n",
    "def gold_sales_per_country():\n",
    "\n",
    "    sales_detail_df = spark.read.table(f\"{silver_schema}.sales_detail\")\n",
    "\n",
    "    segment_performance_df = sales_detail_df \\\n",
    "    .groupBy(\"customer_segment\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_sales\").alias(\"total_sales\"),\n",
    "        F.sum(\"total_profit\").alias(\"total_revenue\"),\n",
    "        F.count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        F.count(\"customer_id\").alias(\"customer_count\"),\n",
    "        F.round(F.avg(\"profit_margin_pct\"), 2).alias(\"avg_profit_margin_pct\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_sales\", F.round(F.col(\"total_sales\"), 2)) \\\n",
    "    .withColumn(\"total_revenue\", F.round(F.col(\"total_revenue\"), 2)) \\\n",
    "    .withColumn(\"revenue_per_customer\", F.round(F.col(\"total_revenue\") / F.col(\"customer_count\"), 2)) \\\n",
    "    .orderBy(F.col(\"total_revenue\").desc())\n",
    "\n",
    "    return segment_performance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
